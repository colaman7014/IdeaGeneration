"""Vercel API å®¢æˆ¶ç«¯æœå‹™"""
import httpx
from typing import Optional

from app.core.config import get_settings


class VercelLLMClient:
    """Vercel API LLM å®¢æˆ¶ç«¯"""
    
    # Vercel AI Gateway ç«¯é»žï¼ˆæ­£ç¢ºç«¯é»žï¼‰
    BASE_URL = "https://ai-gateway.vercel.sh/v1"
    
    def __init__(self):
        self.settings = get_settings()
        self.api_key = self.settings.vercel_api_key
    
    async def complete(
        self,
        prompt: str,
        model: str = "openai/gpt-4o-mini",
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> Optional[str]:
        """å‘¼å« Vercel API å®Œæˆæ–‡å­—ç”Ÿæˆ"""
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        async with httpx.AsyncClient() as client:
            try:
                response = await client.post(
                    f"{self.BASE_URL}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=30.0
                )
                response.raise_for_status()
                
                data = response.json()
                return data.get("choices", [{}])[0].get("message", {}).get("content", "")
                
            except httpx.HTTPStatusError as e:
                raise Exception(f"Vercel API éŒ¯èª¤: {e.response.status_code} - {e.response.text}")
            except httpx.RequestError as e:
                raise Exception(f"ç¶²è·¯è«‹æ±‚éŒ¯èª¤: {str(e)}")
    
    async def extract_tags(self, news_title: str, news_summary: str) -> list[str]:
        """å¾žæ–°èžä¸­æå–æ¨™ç±¤"""
        
        prompt = TAG_EXTRACTION_PROMPT.format(
            title=news_title,
            summary=news_summary
        )
        
        result = await self.complete(
            prompt=prompt,
            max_tokens=200,
            temperature=0.3
        )
        
        if not result:
            return []
        
        # è§£æžæ¨™ç±¤ï¼ˆé æœŸæ ¼å¼ï¼šé€—è™Ÿåˆ†éš”ï¼‰
        tags = [tag.strip() for tag in result.split(",") if tag.strip()]
        return tags[:5]  # æœ€å¤š 5 å€‹æ¨™ç±¤


# AI æ¨™ç±¤æå– Prompt
TAG_EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–°èžåˆ†æžå¸«ã€‚è«‹å¾žä»¥ä¸‹æ–°èžä¸­æå– 3-5 å€‹é—œéµæ¨™ç±¤ã€‚

æ¨™ç±¤æ‡‰è©²ï¼š
1. åæ˜ æ–°èžçš„æ ¸å¿ƒä¸»é¡Œå’Œç”¢æ¥­é ˜åŸŸ
2. åŒ…å«å¯èƒ½æ¿€ç™¼å•†æ¥­éˆæ„Ÿçš„é—œéµè©ž
3. ç°¡æ½”æ˜Žç¢ºï¼Œæ¯å€‹æ¨™ç±¤ 2-4 å€‹å­—

æ–°èžæ¨™é¡Œï¼š{title}
æ–°èžæ‘˜è¦ï¼š{summary}

è«‹ç›´æŽ¥è¼¸å‡ºæ¨™ç±¤ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼Œä¸è¦åŠ ä»»ä½•é¡å¤–èªªæ˜Žã€‚
ç¯„ä¾‹è¼¸å‡ºï¼šäººå·¥æ™ºæ…§, é†«ç™‚ç§‘æŠ€, æ•¸æ“šåˆ†æž, å‰µæ¥­æŠ•è³‡"""


# AI å•†æ¥­æ§‹æƒ³åˆæˆå™¨ Prompt
IDEA_SYNTHESIS_PROMPT = """ä½ æ˜¯ä¸€ä½å‰µæ–°å•†æ¥­é¡§å•ï¼Œå°ˆé–€å°‡çœ‹ä¼¼ç„¡é—œçš„æ–°èžè¶¨å‹¢çµåˆæˆç¨ç‰¹çš„å•†æ¥­æ§‹æƒ³ã€‚

## ä½ çš„ä»»å‹™
æ ¹æ“šä»¥ä¸‹å…©å‰‡æ–°èžçš„æ¨™ç±¤ï¼Œå‰µé€ ä¸€å€‹çµåˆå…©è€…å…ƒç´ çš„å‰µæ–°å•†æ¥­é»žå­ã€‚

## æ–°èž A
æ¨™é¡Œï¼š{news_a_title}
æ¨™ç±¤ï¼š{tags_a}

## æ–°èž B
æ¨™é¡Œï¼š{news_b_title}
æ¨™ç±¤ï¼š{tags_b}

## è¼¸å‡ºæ ¼å¼ï¼ˆè«‹åš´æ ¼éµå®ˆï¼‰
```
é»žå­åç¨±ï¼š[ä¸€å¥è©±æ¨™é¡Œï¼Œæœ€å¤š 20 å­—]

æ¦‚å¿µèªªæ˜Žï¼š
[ç”¨ 2-3 å¥è©±èªªæ˜Žé€™å€‹å•†æ¥­æ§‹æƒ³çš„æ ¸å¿ƒåƒ¹å€¼ä¸»å¼µ]

ç›®æ¨™å®¢ç¾¤ï¼š
[æ˜Žç¢ºæŒ‡å‡ºèª°æœƒç‚ºé€™å€‹ç”¢å“/æœå‹™ä»˜è²»]

ç²åˆ©æ¨¡å¼ï¼š
[èªªæ˜Žå¦‚ä½•è³ºéŒ¢ï¼ŒåŒ…å«å®šåƒ¹ç­–ç•¥]

ç«¶çˆ­å„ªå‹¢ï¼š
[ç‚ºä»€éº¼é€™å€‹æ§‹æƒ³é›£ä»¥è¢«è¤‡è£½]

ç¬¬ä¸€æ­¥è¡Œå‹•ï¼š
[æ˜Žå¤©å°±èƒ½é–‹å§‹åšçš„å…·é«”è¡Œå‹•]
```

è«‹ç¢ºä¿é»žå­å…·æœ‰å¯åŸ·è¡Œæ€§ï¼Œä¸æ˜¯ç©ºæ³›çš„æ§‹æƒ³ã€‚"""


# AI é­”é¬¼å¯©è¨ˆ Promptï¼ˆå£“åŠ›æ¸¬è©¦ï¼‰
DEVIL_AUDIT_PROMPT = """ä½ æ˜¯ä¸€ä½æ¥µåº¦æŒ‘å‰”çš„é¢¨éšªæŠ•è³‡äººï¼Œå°ˆé–€æ‰¾å‡ºå•†æ¥­æ§‹æƒ³çš„æ¼æ´žã€‚

ä½ çš„æ€§æ ¼ï¼š
- å°–é…¸åˆ»è–„ä½†ä¸æƒ¡æ¯’
- ç›´æŽ¥äº†ç•¶ä¸ç¹žå½Žå­
- é—œå¿ƒçš„æ˜¯çœŸå¯¦çš„é¢¨éšªï¼Œä¸æ˜¯ç†è«–ä¸Šçš„å•é¡Œ

## è¦å¯©è¨ˆçš„é»žå­
{idea_content}

## ä½ çš„ä»»å‹™
æå‡º 5 å€‹æœ€ç‹ çš„å•é¡Œï¼ŒæŒ‘æˆ°é€™å€‹æ§‹æƒ³çš„å¯è¡Œæ€§ã€‚

## è¼¸å‡ºæ ¼å¼
æ¯å€‹å•é¡Œä¸€è¡Œï¼Œä»¥ã€ŒðŸ”¥ã€é–‹é ­ï¼Œç°¡çŸ­æœ‰åŠ›ï¼ˆæ¯å€‹å•é¡Œæœ€å¤š 50 å­—ï¼‰ã€‚

ç¯„ä¾‹ï¼š
ðŸ”¥ ä½ æ‰“ç®—æ€Žéº¼ç²å–ç¬¬ä¸€æ‰¹ç”¨æˆ¶ï¼Ÿå†·å•Ÿå‹•å•é¡Œæ€Žéº¼è§£æ±ºï¼Ÿ
ðŸ”¥ ç‚ºä»€éº¼å¤§å…¬å¸ä¸èƒ½æ˜Žå¤©å°±è¤‡è£½ä½ ï¼Ÿ
ðŸ”¥ å–®ä½ç¶“æ¿Ÿåˆç†å—Žï¼Ÿå®¢æˆ¶ç²å–æˆæœ¬ vs çµ‚èº«åƒ¹å€¼ï¼Ÿ"""


def create_llm_client() -> VercelLLMClient:
    """å»ºç«‹ LLM å®¢æˆ¶ç«¯å¯¦ä¾‹"""
    return VercelLLMClient()
